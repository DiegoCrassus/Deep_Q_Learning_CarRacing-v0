{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import sleep\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v0').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1064..1339 -> 275-tiles track\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.886416  , 0.69067097, 0.8968802 ], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "state, reward, _, _ = env.step([0,0,1])\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./\"\n",
    "\n",
    "if os.system(\"cd \" + checkpoint_path) == 512: # caso a pasta de caminho não exista, o os.system retornara 512.\n",
    "    os.system(\"mkdir \"+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\tBox(3,)\n",
      "State Space:\tBox(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\\t{}\".format(env.action_space))\n",
    "print(\"State Space:\\t{}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_memory):\n",
    "        self.__buffer = deque(maxlen=max_memory)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.__buffer.append(experience)\n",
    "        \n",
    "    def get_buffer(self):\n",
    "        return self.__buffer\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.__buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size), size=batch_size)\n",
    "        \n",
    "        return [self.__buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, n_actions, hidden=128, learning_rate=0.0001, shape=(64, 64, 3)):\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = shape\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, *self.shape], name=\"X\")\n",
    "                \n",
    "        # targetQ according to Bellman equation: \n",
    "        # Q = r + gamma*max Q', calculated in the function learn()\n",
    "        self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "        \n",
    "        # Action that was performed\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Normalização do input\n",
    "        self.inputscaled = self.X / 255\n",
    "        \n",
    "        with tf.name_scope(\"conv_1\"):\n",
    "            self.conv1 = tf.layers.conv2d(\n",
    "                inputs=self.inputscaled, filters=32, kernel_size=[5, 5], strides=2,\n",
    "                padding=\"valid\", activation=tf.nn.tanh, use_bias=False, name='conv1')\n",
    "        \n",
    "            self.conv2 = tf.layers.conv2d(\n",
    "                inputs=self.conv1, filters=32, kernel_size=[3, 3], strides=2,\n",
    "                padding=\"valid\", activation=tf.nn.tanh, use_bias=False, name='conv2')\n",
    "            \n",
    "            self.pooling_1 = tf.layers.max_pooling2d(self.conv2, pool_size=(2,2), \n",
    "                                                    strides=2, padding=\"SAME\")\n",
    "            \n",
    "        with tf.name_scope(\"conv_2\"):\n",
    "            \n",
    "        \n",
    "            self.conv3 = tf.layers.conv2d(\n",
    "                inputs=self.pooling_1, filters=64, kernel_size=[5, 5], strides=1,\n",
    "                padding=\"valid\", activation=tf.nn.tanh, use_bias=False, name='conv3')\n",
    "\n",
    "            self.conv4 = tf.layers.conv2d(\n",
    "                inputs=self.conv3, filters=self.hidden, kernel_size=[3, 3], strides=1,\n",
    "                padding=\"valid\", activation=tf.nn.tanh, use_bias=False, name='conv4')\n",
    "            \n",
    "            self.pooling_2 = tf.layers.max_pooling2d(self.conv4, pool_size=(2,2), \n",
    "                                                    strides=1, padding=\"SAME\")\n",
    "\n",
    "        \n",
    "        self.valuestream, self.advantagestream = tf.split(self.pooling_2, 2, 3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        \n",
    "        self.advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream, units=self.n_actions,\n",
    "            name=\"advantage\")\n",
    "        self.value = tf.layers.dense(\n",
    "            inputs=self.valuestream, units=1,\n",
    "            name='value')\n",
    "\n",
    "        # Combining value and advantage into Q-values as described above\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        # Q value of the action that was performed\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action,\n",
    "                                                                     self.n_actions,\n",
    "                                                                     dtype=tf.float32)), axis=1)\n",
    "        # Parameter updates\n",
    "        self.loss_op = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_Q, predictions=self.Q))\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função epsilon para a exploração do cenario (environment).\n",
    "def epsilon_greedy(q_values, step, n_steps):\n",
    "    eps_min = 0.01\n",
    "    eps_max = 1.0\n",
    "    \n",
    "    if step < n_steps * 0.9:\n",
    "        eps_decay_steps = 5000000\n",
    "        epsilon = (max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps))\n",
    "    else:\n",
    "        epsilon = 0.6\n",
    "\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(n_output), epsilon # random action\n",
    "    else:\n",
    "        return np.argmax(q_values), epsilon # optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-06c28c203005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmemory_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpossible_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# Memory relay\n",
    "batch_size = 32\n",
    "memory_size = 128\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "\n",
    "input_shape = env.get_sample_shape()\n",
    "n_output = env.action_space.n\n",
    "hidden = 128\n",
    "\n",
    "done = False\n",
    "n_steps = 120\n",
    "step_test = 100\n",
    "save_steps = 50\n",
    "gamma = 0.99\n",
    "learning_rate = 0.01\n",
    "\n",
    "tensorboard = \"./tensorboard/train/\"\n",
    "\n",
    "os.makedirs(tensorboard, exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: A tensorflow sesson object\n",
    "        replay_memory: A ReplayMemory object\n",
    "        main_dqn: A DQN object\n",
    "        target_dqn: A DQN object\n",
    "        batch_size: Integer, Batch size\n",
    "        gamma: Float, discount factor for the Bellman equation\n",
    "    Returns:\n",
    "        loss: The loss of the minibatch, for tensorboard\n",
    "    Draws a minibatch from the replay memory, calculates the \n",
    "    target Q-value that the prediction Q-value is regressed to. \n",
    "    Then a parameter update is performed on the main DQN.\n",
    "    \"\"\"\n",
    "    # Draw a minibatch from the replay memory\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "    \n",
    "    state = np.array([each[0] for each in batch], ndmin=3)\n",
    "    actions = np.array([each[1] for each in batch])\n",
    "    rewards = np.array([each[2] for each in batch])\n",
    "    done = np.array([each[3] for each in batch])\n",
    "    next_state = np.array([each[4] for each in batch], ndmin=3)\n",
    "\n",
    "    # state, reward, done, next_state = replay_memory.sample(batch_size)\n",
    "    \n",
    "    # The main network estimates which action is best (in the next \n",
    "    # state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.X: next_state})\n",
    "    \n",
    "    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.X: next_state})\n",
    "    double_q = q_vals[range(batch_size), arg_q_max]\n",
    "    \n",
    "    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n",
    "    # if the game is over, targetQ=rewards\n",
    "    target_q = rewards + gamma * double_q\n",
    "    \n",
    "    # Gradient descend step to update the parameters of the main network\n",
    "    loss, _ = session.run([main_dqn.loss_op, main_dqn.train_op],\n",
    "                          feed_dict={main_dqn.X: state,\n",
    "                                     main_dqn.target_Q: target_q,\n",
    "                                     main_dqn.action: actions})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(checkpoint_path, input_shape, num_hidden, n_output,\n",
    "                done, n_steps, save_steps, gamma, lr, env, memory):\n",
    "\n",
    "    with tf.variable_scope('mainDQN'):\n",
    "        MAIN_DQN = DQN(env.action_space.n, hidden, learning_rate)  # (★★)\n",
    "    with tf.variable_scope('targetDQN'):\n",
    "        TARGET_DQN = DQN(env.action_space.n, hidden)  # (★★)\n",
    "\n",
    "    LAYER_IDS = [\"conv1\", \"conv2\", \"conv3\", \"conv4\", \"denseAdvantage\",\n",
    "                 \"denseAdvantageBias\", \"denseValue\", \"denseValueBias\"]\n",
    "\n",
    "    with tf.name_scope('Performance'):\n",
    "        LOSS_PH = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "        LOSS_SUMMARY = tf.summary.scalar('loss', LOSS_PH)\n",
    "        REWARD_PH = tf.placeholder(tf.float32, shape=None, name='reward_summary')\n",
    "        REWARD_SUMMARY = tf.summary.scalar('reward', REWARD_PH)\n",
    "        EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name='evaluation_summary')\n",
    "        EVAL_SCORE_SUMMARY = tf.summary.scalar('evaluation_score', EVAL_SCORE_PH)\n",
    "\n",
    "    PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "   \n",
    "    saver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init) \n",
    "        step = 0\n",
    "        max_reward = np.Infinity\n",
    "        count_reward = 0\n",
    "        \n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        actions_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, n_steps):  \n",
    "            \n",
    "            episode_reward_sum = 0\n",
    "\n",
    "            state = env.reset() # inicialização do jogo.\n",
    "            \n",
    "            step += 1 # Incremento na variavel step.\n",
    "            action = env.action_space.sample()\n",
    "            actions_list.append(action)\n",
    "            \n",
    "            state, reward, done, next_state = env.step(action) # inicio das variaveis para o calculo do Q-value\n",
    "            memory.add((state, action, reward, done, next_state))\n",
    "            \n",
    "            while not done:                \n",
    "                    \n",
    "                q_value = sess.run(TARGET_DQN.best_action, feed_dict={TARGET_DQN.X: [state]})\n",
    "                img_state = env.get_state()\n",
    "                # Ação para o q_value do estado anterior.\n",
    "                action, epsilon = epsilon_greedy(q_value, i, n_steps)\n",
    "                \n",
    "                # action = possible_actions[choice]\n",
    "                state, reward, done, next_state = env.step(action)\n",
    "                \n",
    "                #return None\n",
    "                \n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                memory.add((state, action, reward, done, next_state))\n",
    "                \n",
    "                loss = learn(sess, memory, MAIN_DQN, TARGET_DQN, batch_size, gamma)\n",
    "                loss_list.append(loss)\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                # Ignora isso, é só um print...\n",
    "                print(\"State: {}\\tReward: {}\\tAction: {}\\tepslon: {:.2f}\\nTraining step: {}/{}\\tLoss: {:.4f}\\tQ_value: {}\".format(\n",
    "                    img_state.split(\"_\")[0], reward, action, epsilon, i, n_steps, loss, q_value[0]))\n",
    "                \n",
    "                # And save regularly\n",
    "                if step % save_steps == 0 and done == True:\n",
    "                    saver.save(sess, checkpoint_path)\n",
    "                      \n",
    "            rewards.append(episode_reward_sum)\n",
    "            \n",
    "            # Scalar summaries for tensorboard\n",
    "            summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                            feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                       REWARD_PH:np.mean(rewards[-100:])})\n",
    "\n",
    "            SUMM_WRITER.add_summary(summ)\n",
    "            # Histogramm summaries for tensorboard\n",
    "            summ_param = sess.run(PARAM_SUMMARIES)\n",
    "            SUMM_WRITER.add_summary(summ_param)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(memory_size)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "\n",
    "    # Get the next_state, the rewards, done by taking a random action\n",
    "    action = env.action_space.sample()\n",
    "    # action = possible_actions[choice]\n",
    "\n",
    "    state, reward, done, next_state = env.step(action)\n",
    "\n",
    "    # If the episode is finished (we're dead 3x)\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(env.observation_space.shape[0])\n",
    "\n",
    "        # Add experience to memory\n",
    "        if type(action) == np.ndarray:\n",
    "            memory.add((state, action, reward, done, next_state))\n",
    "\n",
    "        # Start a new episode\n",
    "        state = env.reset()\n",
    "\n",
    "        # Stack the frames\n",
    "        # state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        if type(action) == np.ndarray:\n",
    "            memory.add((state, action, reward, done, next_state))\n",
    "\n",
    "        # Our new state is now the next_state\n",
    "        # state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpeza de possiveis grafos carregados na memoria.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inicialização do treino do modelo DQN\n",
    "teste = train_model(checkpoint_path, input_shape, hidden, n_output,\n",
    "           done, n_steps, save_steps, gamma, learning_rate, env, memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-cpu)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
